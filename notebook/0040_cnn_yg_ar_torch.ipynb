{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9b0c144-042d-47aa-b2b6-94b9888d5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deb78370-8250-42f4-9eff-72236f96aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_map(labels):\n",
    "    label_set = set()\n",
    "    for lt in labels:\n",
    "        label_set.add(lt)\n",
    "\n",
    "    label_map = {}\n",
    "    count = 0\n",
    "    for l in label_set:\n",
    "        label_map[l] = count\n",
    "        count += 1\n",
    "        \n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d189fd08-4a09-4414-9540-bc4d8eb46238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # part 1 of vgg-16\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d(p=0.3)\n",
    "        self.norm2a = nn.BatchNorm2d(32)\n",
    "        self.norm2b = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # part 2 of vgg-16\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.conv4_drop = nn.Dropout2d(p=0.3)\n",
    "        self.norm4a = nn.BatchNorm2d(64)\n",
    "        self.norm4b = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # part 3 of vgg-16\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.conv7 = nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.conv7_drop = nn.Dropout2d(p=0.3)\n",
    "        self.norm7a = nn.BatchNorm2d(128)\n",
    "        self.norm7b = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # part 4 of vgg-16\n",
    "        self.conv8 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        self.conv9 = nn.Conv2d(256, 256, kernel_size=3)\n",
    "        self.conv10 = nn.Conv2d(256, 256, kernel_size=3)\n",
    "        self.conv10_drop = nn.Dropout2d(p=0.3)\n",
    "        self.norm10a = nn.BatchNorm2d(256)\n",
    "        self.norm10b = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # # part 5 of vgg-16\n",
    "        # self.conv11 = nn.Conv2d(20, 20, kernel_size=3)\n",
    "        # self.conv12 = nn.Conv2d(20, 20, kernel_size=3)\n",
    "        # self.conv13 = nn.Conv2d(20, 20, kernel_size=3)\n",
    "        # self.conv13_drop = nn.Dropout2d(p=0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4096, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2_drop(self.conv2(x))\n",
    "        x = self.norm2a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm2b(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.norm4a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm4b(x)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.norm7a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm7b(x)\n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = self.norm10a(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.norm10b(x)\n",
    "        \n",
    "        # x = F.relu(self.conv11(x))\n",
    "        # x = F.relu(self.conv12(x))\n",
    "        # x = F.relu(F.max_pool2d(self.conv13_drop(self.conv13(x)), 2))\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 4096)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4346fd8f-7343-4636-9b25-7205afc59bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        epoch,\n",
    "        model,\n",
    "        loss_func,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        log_interval,\n",
    "        save_path\n",
    "):  \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "\n",
    "            trained_count = batch_idx * len(data)\n",
    "            total_count = len(train_loader.dataset)\n",
    "            batch_percent = int(100 * batch_idx / len(train_loader))\n",
    "            loss_val = loss.item() / len(data)\n",
    "            print(\n",
    "                f'Train Epoch: {epoch} ' + \n",
    "                f'[{trained_count}/{total_count} ({batch_percent}%)]' + \n",
    "                f'\\tLoss: {loss_val:.6f}'\n",
    "            )\n",
    "\n",
    "    torch.save(model.state_dict(), osp.join(save_path, f\"model_{epoch}.pt\"))\n",
    "    torch.save(optimizer.state_dict(), osp.join(save_path, f\"opt_{epoch}.pt\"))\n",
    "    \n",
    "    return loss.item() / len(data)\n",
    "\n",
    "\n",
    "def test(\n",
    "    model,\n",
    "    loss_func,\n",
    "    test_loader,\n",
    "    test_type=\"Validation\"\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_func(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        print(\n",
    "            f'{test_type} Average loss: {test_loss:.4f}, ' +\n",
    "            f'Accuracy: {correct}/{len(test_loader.dataset)} ' + \n",
    "            f'({100.*correct/len(test_loader.dataset):.0f}%)'\n",
    "        )\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dc0ce214-6cf9-495b-900d-08950a123aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bf5c2513d0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_valid = 1000\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "save_path = \"C:/Users/aphri/Documents/t0002/pycharm/data/yg_ar/cnn/res0001\"\n",
    "\n",
    "if not osp.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "669c236f-053e-42e3-a893-5f190ad9e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebula.data.yg_ar.setup_data_image_hard import read_data\n",
    "from nebula.common import to_scale_one, write_pickle, read_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1fe934af-4abe-4eae-b7a3-c08352cc3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"C:/Users/aphri/Documents/t0002/pycharm/data/yg_ar/image_hard_df.pkl\"\n",
    "random_seed = 1\n",
    "df, train_df, test_df, valid_df = read_data(df_path, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3191e21f-c513-4e61-b9d9-1907900ad11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_a = create_label_map(df[\"label_a\"])\n",
    "label_map_at = create_label_map(df[\"label_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "446c9584-3c40-405c-b2a5-599f72841fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[\"image\"].apply(lambda x: np.array([x.astype(np.float32)/225.0])).to_list()\n",
    "train_y_a = train_df[\"label_a\"].map(label_map_a).to_list()\n",
    "train_y_at = train_df[\"label_at\"].map(label_map_at).to_list()\n",
    "train_loader_a = torch.utils.data.DataLoader(tuple(zip(train_x, train_y_a)), batch_size=batch_size_train, shuffle=True)\n",
    "train_loader_at = torch.utils.data.DataLoader(tuple(zip(train_x, train_y_at)), batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3967b8f9-784e-4586-9dae-c72c0aa7814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = valid_df[\"image\"].apply(lambda x:  np.array([x.astype(np.float32)/225.0])).to_list()\n",
    "valid_y_a = valid_df[\"label_a\"].map(label_map_a).to_list()\n",
    "valid_y_at = valid_df[\"label_at\"].map(label_map_at).to_list()\n",
    "valid_loader_a = torch.utils.data.DataLoader(tuple(zip(valid_x, valid_y_a)), batch_size=batch_size_valid, shuffle=True)\n",
    "valid_loader_at = torch.utils.data.DataLoader(tuple(zip(valid_x, valid_y_at)), batch_size=batch_size_valid, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9221bfea-71c8-4816-9e18-dd74d126ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_df[\"image\"].apply(lambda x:  np.array([x.astype(np.float32)/225.0])).to_list()\n",
    "test_y_a = test_df[\"label_a\"].map(label_map_a).to_list()\n",
    "test_y_at = test_df[\"label_at\"].map(label_map_at).to_list()\n",
    "test_loader_a = torch.utils.data.DataLoader(tuple(zip(test_x, test_y_a)), batch_size=batch_size_test, shuffle=True)\n",
    "test_loader_at = torch.utils.data.DataLoader(tuple(zip(test_x, test_y_at)), batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68f1d7-45bd-4849-a8d3-9900fc44dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aphri\\miniconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Average loss: 2.3028, Accuracy: 176/1760 (10%)\n",
      "Train Epoch: 1 [0/16080 (0%)]\tLoss: 2.306470\n",
      "Train Epoch: 1 [640/16080 (3%)]\tLoss: 3.753809\n",
      "Train Epoch: 1 [1280/16080 (7%)]\tLoss: 2.838478\n",
      "Train Epoch: 1 [1920/16080 (11%)]\tLoss: 2.742681\n",
      "Train Epoch: 1 [2560/16080 (15%)]\tLoss: 3.123515\n",
      "Train Epoch: 1 [3200/16080 (19%)]\tLoss: 2.524794\n",
      "Train Epoch: 1 [3840/16080 (23%)]\tLoss: 2.635178\n",
      "Train Epoch: 1 [4480/16080 (27%)]\tLoss: 2.368749\n",
      "Train Epoch: 1 [5120/16080 (31%)]\tLoss: 2.317142\n",
      "Train Epoch: 1 [5760/16080 (35%)]\tLoss: 2.366688\n",
      "Train Epoch: 1 [6400/16080 (39%)]\tLoss: 2.383743\n",
      "Train Epoch: 1 [7040/16080 (43%)]\tLoss: 2.301859\n",
      "Train Epoch: 1 [7680/16080 (47%)]\tLoss: 2.303848\n",
      "Train Epoch: 1 [8320/16080 (51%)]\tLoss: 2.410814\n",
      "Train Epoch: 1 [8960/16080 (55%)]\tLoss: 2.304687\n",
      "Train Epoch: 1 [9600/16080 (59%)]\tLoss: 2.313536\n",
      "Train Epoch: 1 [10240/16080 (63%)]\tLoss: 2.303791\n",
      "Train Epoch: 1 [10880/16080 (67%)]\tLoss: 2.303215\n",
      "Train Epoch: 1 [11520/16080 (71%)]\tLoss: 2.319545\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss(size_average=False)\n",
    "\n",
    "train_res = []\n",
    "test_res = []\n",
    "\n",
    "test(\n",
    "    model=model,\n",
    "    loss_func=loss_func,\n",
    "    test_loader=valid_loader_a\n",
    ")\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = train(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        loss_func=loss_func,\n",
    "        train_loader=train_loader_a,\n",
    "        optimizer=optimizer,\n",
    "        log_interval=log_interval,\n",
    "        save_path=save_path\n",
    "    )\n",
    "    train_res.append((epoch, train_loss))\n",
    "\n",
    "    test_loss = test(\n",
    "        model=model,\n",
    "        loss_func=loss_func,\n",
    "        test_loader=valid_loader_a\n",
    "    )\n",
    "    test_res.append((epoch, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58390a0-45da-4e87-a834-37719c413ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
